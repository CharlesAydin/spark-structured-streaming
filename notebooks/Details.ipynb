{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sparkDummy = spark\n",
    "import sparkDummy.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with DStream\n",
    "\n",
    "1. DStream made it hard to deal with late data because the DStream was just discretized batches\n",
    "1. Same API between batch Datasets and structured streaming\n",
    "1. Better end-to-end guarantees to handle fault-tolerant data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benefits of Type Safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benefits of Tungsten\n",
    "\n",
    "While Spark has traditionally been focused on optimizing for inter-computer network IO Efficiency.  This was the major bottle neck in mapreduce and other traditional distributed computing systems.  At this point, Spark is now intra-computer memory and CPU bound.\n",
    "\n",
    "1. **Customized Memory Management:** Spark understands its own memory allocation needs better than the generic JVM garbage collector.  Tungsten takes advantage of this by using `sun.misc.Unsafe`, which exposes C-Style off-heap memory access.  The result is fewer unecessary garbage colleciton events and improved performance.\n",
    "1. **Binary Encoding:** Spark traditionally needed to serialized data to JVM objects, it now uses the Tungsten binary encoding.  This has two major advantages.  First, it decreases the memory footprint.  While \"ABCD\" would take 4 bytes of UTF-8 to encode, it would be stored using 48 bytes as a JVM object.  Second, instead of serializing into objects, it's able to perform many actions directly on the the raw binary encoding, reducing the computational overhead for serialization / deserialization.\n",
    "1. **Code Generation:** Spark historically used generic JVM function evaluation.  Given the virtual function lookups, automated boxing of primitive types, and other JVM overhead, this dramatically slows down the computation.  By using type information, Tungsten is able to generate byte code and speed up performance.\n",
    "1. **Cache-aware Computation:** Tungsten lays out its memory in a way that takes advantage of CPU cache locality to reduce cache spilling and speed up computations.\n",
    "\n",
    "For more information, check out [this blog post](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html) or [this presentation](http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tungsten In Action\n",
    "\n",
    "The following two operations count a million integers in memory.\n",
    "- The first uses RDDs and must serialize all the objects into Java Objects.\n",
    "- The second use sTungsten and uses a more compact binary encoding.\n",
    "\n",
    "Tungsten saves a factor of nearly 4 on memory:\n",
    "\n",
    "![Tungsten Memory](images/Tungsten_Memory.png)\n",
    "\n",
    "To reproduce the example, run the following code and goto your Spark UI Viewer.  You can launch a new shell using `make spark-shell` (recommended), this will be at [http://localhost:5050/storage/](http://localhost:5050/storage/).  The default Spark UI port is 4040."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val million = 0 until math.pow(10, 6).toInt\n",
    "\n",
    "// Using RDDs\n",
    "sc.parallelize(million).cache.count\n",
    "\n",
    "// Using Tungsten\n",
    "million.toDF.cache.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benefits of Catalyst\n",
    "\n",
    "https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\n",
    "\n",
    "Optimizations include:\n",
    "1. **Constant Folding:** Evaluating constant expressions at compile time rather than at run time.\n",
    "1. **Predicate Pushdown:** Running operations that reduce the dataload (e.g. selecting columns or filtering rows) earlier in the query.  This reduces the amount of data that needs to be \n",
    "1. **Boolean Expression Simplification:** Simplifies boolean expressions\n",
    "1. **Pipelining Operations:** Combines multiple projection and filter operations into a single map operation.\n",
    "1. **Cost-based Optimization:** Spark actually builds multiple plans to compute a query, computes their cost, and chooses the cheapest one.\n",
    "1. **Code Generation:** The final operational plan is transformed into optimized Java bytecode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with Streaming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
