{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark, Datasets, and Structured Streaming\n",
    "\n",
    "- RDD\n",
    "- Resilient\n",
    "- Distributed\n",
    "- In RAM\n",
    "- etc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark in Action\n",
    "\n",
    "Below, we give a simple example of a Spark Program that counts the words in Shakespeare's Othello.  Notice how much more concise the code is compared to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i,803)\n",
      "(and,769)\n",
      "(the,756)\n",
      "(to,572)\n",
      "(of,470)\n",
      "(a,441)\n",
      "(my,427)\n",
      "(that,337)\n",
      "(you,335)\n",
      "(in,319)\n",
      "(iago,299)\n",
      "(othello,292)\n",
      "(not,278)\n",
      "(is,276)\n",
      "(it,244)\n",
      "(with,221)\n",
      "(for,219)\n",
      "(be,208)\n",
      "(your,207)\n",
      "(he,205)\n"
     ]
    }
   ],
   "source": [
    "// Load Data\n",
    "val lines = sc.textFile(\"data/othello.txt\")\n",
    "\n",
    "// Count words\n",
    "val counts = (lines.flatMap(line => line.split(\"\\\\s+\"))\n",
    "    .map(word => (word.toLowerCase, 1))\n",
    "    .reduceByKey(_ + _))\n",
    "    \n",
    "(counts.sortBy(_._2, ascending=false)\n",
    "    .take(20)\n",
    "    .foreach(println))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exercise**:\n",
    "\n",
    "Spark interface allows you to do everything SQL might do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A More Complex Spark Example\n",
    "\n",
    "One common use case is joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Denise,10.0)\n",
      "(Charlie,40.0)\n",
      "(Amy,70.0)\n",
      "(Bob,20.0)\n"
     ]
    }
   ],
   "source": [
    "case class User(id: Int, name: String, email: String, country: String)\n",
    "case class Transaction(userid: Int, product: String, cost: Double)\n",
    "\n",
    "val users = (sc.textFile(\"data/users.csv\")\n",
    "    .map{ t =>\n",
    "        val p = t.split(\",\")\n",
    "        User(p(0).toInt, p(1), p(2), p(3))\n",
    "    })\n",
    "        \n",
    "val transactions = (sc.textFile(\"data/transactions.csv\")\n",
    "    .map{ t =>\n",
    "        val p = t.split(\",\")\n",
    "        Transaction(p(0).toInt, p(1), p(2).toDouble)\n",
    "    })\n",
    "\n",
    "val userTransactions = (users.map(u => (u.id, u.name))\n",
    "    .join(transactions.map(t => (t.userid, t.cost)))\n",
    ").values\n",
    "\n",
    "val transactionsByUsers = userTransactions.reduceByKey(_ + _).collect\n",
    "\n",
    "transactionsByUsers.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Notice that there's a user (\"Edward\") who did not make a purchase.  He disappeared from `transactionsByUsers` because we were doing an (inner) `join`.  Instead, do a `leftOuterJoin` so we have a record that he spent nothing.  Notice that this returns an `Option`.  You may find `flatMap` useful for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark and Datasets\n",
    "\n",
    "- RDDs are slow because of using Java serialization (can use Kryo)\n",
    "- Dataframes use Catalyst but don't provide type safety.\n",
    "\n",
    "http://www.agildata.com/apache-spark-rdd-vs-dataframe-vs-dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example with Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Normally, run `import spark.implicits._`\n",
    "// There's a namespace collision in a Spark Notebook that requires this\n",
    "\n",
    "val sparkDummy = spark\n",
    "import sparkDummy.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|  value|count(1)|\n",
      "+-------+--------+\n",
      "|      i|     803|\n",
      "|    and|     769|\n",
      "|    the|     756|\n",
      "|     to|     572|\n",
      "|     of|     470|\n",
      "|      a|     441|\n",
      "|     my|     427|\n",
      "|   that|     337|\n",
      "|    you|     335|\n",
      "|     in|     319|\n",
      "|   iago|     299|\n",
      "|othello|     292|\n",
      "|    not|     278|\n",
      "|     is|     276|\n",
      "|     it|     244|\n",
      "|   with|     221|\n",
      "|    for|     219|\n",
      "|     be|     208|\n",
      "|   your|     207|\n",
      "|     he|     205|\n",
      "+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Load Data\n",
    "val text = (spark.read\n",
    "    .text(\"data/othello.txt\").as[String])\n",
    "\n",
    "// Count words\n",
    "val counts = (text.flatMap(line => line.split(\"\\\\s+\"))\n",
    "    .groupByKey(_.toLowerCase)\n",
    "    .count)\n",
    "\n",
    "// Display most common\n",
    "counts.orderBy($\"count(1)\" desc).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards Structured Streaming\n",
    "\n",
    "Structured Streaming is based on datasets.\n",
    "\n",
    "- word count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+--------+\n",
      "|  value|count(1)|\n",
      "+-------+--------+\n",
      "|      i|     803|\n",
      "|    and|     769|\n",
      "|    the|     756|\n",
      "|     to|     572|\n",
      "|     of|     470|\n",
      "|      a|     441|\n",
      "|     my|     427|\n",
      "|   that|     337|\n",
      "|    you|     335|\n",
      "|     in|     319|\n",
      "|   iago|     299|\n",
      "|othello|     292|\n",
      "|    not|     278|\n",
      "|     is|     276|\n",
      "|     it|     244|\n",
      "|   with|     221|\n",
      "|    for|     219|\n",
      "|     be|     208|\n",
      "|   your|     207|\n",
      "|     he|     205|\n",
      "+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Load Data\n",
    "val text = (spark.readStream\n",
    "    .text(\"data/othello.txt\")\n",
    "    .as[String])\n",
    "    \n",
    "// Count words\n",
    "val counts = (text.flatMap(line => line.split(\"\\\\s+\"))\n",
    "    .groupByKey(_.toLowerCase)\n",
    "    .count)\n",
    "    \n",
    "val query = (counts\n",
    "    .orderBy($\"count(1)\" desc)\n",
    "    .writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .format(\"console\")\n",
    "    .start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
