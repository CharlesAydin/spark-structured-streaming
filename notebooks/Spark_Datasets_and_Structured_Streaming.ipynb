{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark, Datasets, and Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Overview\n",
    "\n",
    "[Spark](http://spark.apache.org/) is a platform for distributed computation that has several great features:\n",
    "\n",
    "1. Transparently processes data on multiple nodes in the cloud while giving the programmer an API that's little more complex than Scala's `Seq` API.\n",
    "1. Resiliently handles failures by restarting processess.\n",
    "1. Spills data to disk as necessary but prefers to intelligently cache in ram for faster processing.\n",
    "1. Java, Scala, Python, R, and SQL APIs (although we will primarily be using the Scala one).\n",
    "1. The same Spark code can run in standalone (single-node cluster for development), Hadoop, mesos, the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Wordcount Using RDD Example\n",
    "\n",
    "In order to speak about Spark Structured Streaming, we need to first introduce the basics of batch jobs in Spark and highlight the main features of the batch API.  In addition, this will be useful when we look at Spark discretized streams later on.\n",
    "\n",
    "The key feature of the batch API is an resilient distributed dataset (RDD).  These share a similar API to Scala `Seq` and transparently handles large datasets data by conveniently abstracting the distributing computation load.\n",
    "\n",
    "Below, we give a simple example of a Spark Program that counts the words in Shakespeare's Othello.  Notice how much more concise the code is compared to the typical [wordcount in Mapreduce](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v2.0) that's 133 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i,803)\n",
      "(and,769)\n",
      "(the,756)\n",
      "(to,572)\n",
      "(of,470)\n",
      "(a,441)\n",
      "(my,427)\n",
      "(that,337)\n",
      "(you,335)\n",
      "(in,319)\n",
      "(iago,299)\n",
      "(othello,292)\n",
      "(not,278)\n",
      "(is,276)\n",
      "(it,244)\n",
      "(with,221)\n",
      "(for,219)\n",
      "(be,208)\n",
      "(your,207)\n",
      "(he,205)\n"
     ]
    }
   ],
   "source": [
    "// Load data\n",
    "val lines = sc.textFile(\"data/othello.txt\")\n",
    "\n",
    "// Count words\n",
    "val counts = (lines.flatMap(line => line.split(\"\\\\s+\"))\n",
    "    .map(word => (word.toLowerCase, 1))\n",
    "    .reduceByKey(_ + _))\n",
    "    \n",
    "// Sort and print top 20\n",
    "(counts.sortBy(_._2, ascending=false)\n",
    "    .take(20)\n",
    "    .foreach(println))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Environments for Running Spark\n",
    "\n",
    "There are two contexts for running Spark:\n",
    "\n",
    "1. We will primarily be demonstrating the \"REPL\" method.  You can access the repl method by running any of the code cells in these notebooks or by running `spark-shell` from the bash command line.  Notebooks are great for dedactic, exploratory, and presentation purposes.  The shell is also great  for exploration.\n",
    "\n",
    "2. You can also write Spark jobs as a program to be packaged up and run as a standalone jar application.  This requires using build tools (e.g. SBT) and a reasonable amount of overhead.  We'll explore how to do this in the final notebook.  This is great for production code.\n",
    "\n",
    "For (2), you'll need to create your own `SparkContext` and `SparkSession`.  In (1), these are provided as global variables named `sc` and `spark`.  You'll access spark functionality through these two objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Wordcount Using Scala Example\n",
    "\n",
    "Rather than using the complex mapreduce paradigm, Spark's API resembles the the Scala `Seq` API.  For example, the code below uses native Scala methods to implement a (much less powerful) word count.  The amazing thing about Spark is that it allows us distributed computation with the same level of API complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i,803)\n",
      "(and,769)\n",
      "(the,756)\n",
      "(to,572)\n",
      "(of,470)\n",
      "(a,441)\n",
      "(my,427)\n",
      "(that,337)\n",
      "(you,335)\n",
      "(in,319)\n",
      "(iago,299)\n",
      "(othello,292)\n",
      "(not,278)\n",
      "(is,276)\n",
      "(it,244)\n",
      "(with,221)\n",
      "(for,219)\n",
      "(be,208)\n",
      "(your,207)\n",
      "(he,205)\n"
     ]
    }
   ],
   "source": [
    "import scala.io.Source\n",
    "\n",
    "// Load data\n",
    "val lines = (Source.fromFile(\"data/othello.txt\")\n",
    "    .getLines\n",
    " \n",
    "// Count words\n",
    "val counts = (lines.flatMap(line => line.split(\"\\\\s+\"))\n",
    "    .toSeq\n",
    "    .groupBy(_.toLowerCase)\n",
    "    .mapValues(_.length))\n",
    "       \n",
    "// Sort and print top 20\n",
    "(counts.toSeq\n",
    "    .sortBy(_._2)\n",
    "    .reverse\n",
    "    .take(20)\n",
    "    .foreach(println))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exercise**: Use the `filter` method on RDDs to only count the number of instances of lower case words using Spark.  Verify your answer with the native Scala implementation using `Seq`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark and Datasets\n",
    "\n",
    "While RDDs are great, they lacked several things.\n",
    "- RDDs are slow because of Java serialization (this can be sped up by using Kryo).\n",
    "- Dataframes provide named columns and identical rows, like [Dataframes in R](http://www.r-tutor.com/r-introduction/data-frame) or Dataframes in [Python Pandas](http://pandas.pydata.org/).  It benefits from many advanced optimizations.  Unfortunately, we sacrifice typesafety.\n",
    "- Datasets provides an API which has many of those options as before.  They have many of the same performance benefits without needing to sacrifice typesafety."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Wordcount Using Datasets Example\n",
    "\n",
    "Datasets make heavy use of implicits, which we will have to import.  The API also uses column types which are strings prefixed by `$` or called from the method `.col`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Normally, run `import spark.implicits._`\n",
    "// There's a namespace collision in a Spark Notebook that requires this\n",
    "\n",
    "val sparkDummy = spark\n",
    "import sparkDummy.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|  value|count(1)|\n",
      "+-------+--------+\n",
      "|      i|     803|\n",
      "|    and|     769|\n",
      "|    the|     756|\n",
      "|     to|     572|\n",
      "|     of|     470|\n",
      "|      a|     441|\n",
      "|     my|     427|\n",
      "|   that|     337|\n",
      "|    you|     335|\n",
      "|     in|     319|\n",
      "|   iago|     299|\n",
      "|othello|     292|\n",
      "|    not|     278|\n",
      "|     is|     276|\n",
      "|     it|     244|\n",
      "|   with|     221|\n",
      "|    for|     219|\n",
      "|     be|     208|\n",
      "|   your|     207|\n",
      "|     he|     205|\n",
      "+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Load Data\n",
    "val text = (spark.read\n",
    "    .text(\"data/othello/part*\")\n",
    "    .as[String])\n",
    "\n",
    "// Count words\n",
    "val counts = (text.flatMap(line => line.split(\"\\\\s+\"))\n",
    "    .groupByKey(_.toLowerCase)\n",
    "    .count)\n",
    "\n",
    "// Display most common\n",
    "counts.orderBy($\"count(1)\" desc).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Data Using Spark Datasets\n",
    "\n",
    "So far, we have covered `map`, `flatMap`, and `filter`, represent three major operations on RDDs (which are also used in streaming).  The final operation we will cover is loading and joining structured data.\n",
    "\n",
    "**Loading Data:** Datasets supports many nice helper functions for loading data.\n",
    "1. In the code below, we infer the data schema, read from headers, and cast to a case class.\n",
    "1. It also supports a SQL-like join syntax via the special `===` operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,name,email,country\n",
      "0,Amy,amy@example.com,EN\n",
      "1,Bob,bob@example.com,EN\n",
      "2,Charlie,charlie@example.com,FR\n",
      "3,Denise,denise@example.com,FR\n",
      "4,Edward,edward@example.com,DE\n"
     ]
    }
   ],
   "source": [
    "import sys.process._\n",
    "\n",
    "\"more data/users.csv\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|sum(cost)|\n",
      "+-------+---------+\n",
      "| Denise|       10|\n",
      "|    Amy|       70|\n",
      "|Charlie|       40|\n",
      "|    Bob|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case class User(id: Int, name: String, email: String, country: String)\n",
    "case class Transaction(userid: Int, product: String, cost: Double)\n",
    "\n",
    "val users = (spark.read\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(\"data/users.csv\")\n",
    "    .as[User])\n",
    "    \n",
    "val transactions = (spark.read\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(\"data/transactions.csv\")\n",
    "    .as[Transaction])\n",
    "\n",
    "(users.join(transactions, users.col(\"id\") === transactions.col(\"userid\"))\n",
    "    .groupBy($\"name\")\n",
    "    .sum(\"cost\")\n",
    "    .show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Notice that there's a user (\"Edward\") who did not make a purchase.  He disappeared from `transactionsByUsers` because we were doing an (inner) `join`.  Instead, do a left outer join so we have a record that he spent nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Streaming Overview\n",
    "\n",
    "So far, we have only talked about batch operations in the form of RDDs and Datasets.  We will now introduce Spark Structured Streaming.  The easiest way to think about Spark Structured Streaming as a dataset with an infintie number of rows.  As more data is added, more rows are appended to the dataset.\n",
    "\n",
    "![](images/structured-streaming-stream-as-a-table.png)\n",
    "\n",
    "This interface has a number of benefits, primary among them is that the API for streaming is almost exactly the same as the API for datasets.\n",
    "\n",
    "With Spark Structured Streaming, we define a trigger which occurs regularly (for example, every second).  When the trigger fires, more input (table rows) are appended to the \"dataset\".  We can then write queries (for examlpe, wordcount) against the entire (ever growing) \"dataset\".  Of course, the queries are computed incrementally so that we only have to process the new data at each time point.\n",
    "\n",
    "![](images/structured-streaming-model.png)\n",
    "\n",
    "Finally, it's worth keeping in mind that with streaming, spark is still distributing the workload across multiple nodes.  Your data is essentailly distributed along two dimensions: time (as the data streams in) and nodes.\n",
    "\n",
    "<!--- Images are from https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming Wordcount Example\n",
    "\n",
    "Below, we have chunked Othello into multiple parts.  Structured Streaming provies a useful option for reading in each part on a separate trigger, allowing us to fake a stream from a file.\n",
    "\n",
    "Note how similar the Structured Streaming API is to the dataset API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+--------+\n",
      "|value|count(1)|\n",
      "+-----+--------+\n",
      "|  the|     206|\n",
      "|  and|     191|\n",
      "|   of|     166|\n",
      "|    i|     163|\n",
      "|   to|     147|\n",
      "|   my|     109|\n",
      "|    a|     105|\n",
      "|   in|      75|\n",
      "| with|      70|\n",
      "|   is|      68|\n",
      "| that|      59|\n",
      "| your|      57|\n",
      "|  you|      56|\n",
      "|  for|      53|\n",
      "|   it|      52|\n",
      "| have|      47|\n",
      "|  not|      46|\n",
      "|   be|      45|\n",
      "|   do|      42|\n",
      "|  but|      40|\n",
      "+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+--------+\n",
      "|value|count(1)|\n",
      "+-----+--------+\n",
      "|  the|     407|\n",
      "|  and|     376|\n",
      "|    i|     293|\n",
      "|   to|     286|\n",
      "|   of|     268|\n",
      "|    a|     203|\n",
      "|   my|     158|\n",
      "|   in|     147|\n",
      "| that|     134|\n",
      "|   is|     129|\n",
      "|  you|     112|\n",
      "| with|     110|\n",
      "|  for|     109|\n",
      "| iago|      98|\n",
      "| your|      97|\n",
      "|  not|      94|\n",
      "|   it|      90|\n",
      "| this|      86|\n",
      "| have|      83|\n",
      "|   be|      82|\n",
      "+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+--------+\n",
      "|  value|count(1)|\n",
      "+-------+--------+\n",
      "|    the|     518|\n",
      "|    and|     503|\n",
      "|      i|     461|\n",
      "|     to|     415|\n",
      "|     of|     349|\n",
      "|      a|     282|\n",
      "|     my|     261|\n",
      "|   that|     208|\n",
      "|     in|     201|\n",
      "|    you|     177|\n",
      "|     is|     172|\n",
      "|    not|     170|\n",
      "|   iago|     168|\n",
      "|    for|     154|\n",
      "|   with|     148|\n",
      "|   your|     141|\n",
      "|     be|     140|\n",
      "|     it|     139|\n",
      "|othello|     134|\n",
      "|    but|     119|\n",
      "+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+--------+\n",
      "|  value|count(1)|\n",
      "+-------+--------+\n",
      "|    the|     634|\n",
      "|    and|     632|\n",
      "|      i|     619|\n",
      "|     to|     492|\n",
      "|     of|     409|\n",
      "|      a|     347|\n",
      "|     my|     346|\n",
      "|   that|     265|\n",
      "|     in|     260|\n",
      "|    you|     257|\n",
      "|   iago|     243|\n",
      "|    not|     229|\n",
      "|     is|     220|\n",
      "|othello|     213|\n",
      "|   with|     191|\n",
      "|     it|     187|\n",
      "|   your|     179|\n",
      "|    for|     178|\n",
      "|     be|     177|\n",
      "|     do|     160|\n",
      "+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------+--------+\n",
      "|  value|count(1)|\n",
      "+-------+--------+\n",
      "|      i|     783|\n",
      "|    and|     748|\n",
      "|    the|     725|\n",
      "|     to|     561|\n",
      "|     of|     455|\n",
      "|      a|     430|\n",
      "|     my|     423|\n",
      "|   that|     324|\n",
      "|    you|     322|\n",
      "|     in|     303|\n",
      "|   iago|     293|\n",
      "|othello|     281|\n",
      "|    not|     273|\n",
      "|     is|     270|\n",
      "|     it|     238|\n",
      "|   with|     219|\n",
      "|    for|     214|\n",
      "|     be|     202|\n",
      "|   your|     201|\n",
      "|     do|     198|\n",
      "+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-------+--------+\n",
      "|  value|count(1)|\n",
      "+-------+--------+\n",
      "|      i|     803|\n",
      "|    and|     769|\n",
      "|    the|     756|\n",
      "|     to|     572|\n",
      "|     of|     470|\n",
      "|      a|     441|\n",
      "|     my|     427|\n",
      "|   that|     337|\n",
      "|    you|     335|\n",
      "|     in|     319|\n",
      "|   iago|     299|\n",
      "|othello|     292|\n",
      "|    not|     278|\n",
      "|     is|     276|\n",
      "|     it|     244|\n",
      "|   with|     221|\n",
      "|    for|     219|\n",
      "|     be|     208|\n",
      "|   your|     207|\n",
      "|     he|     205|\n",
      "+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Load data\n",
    "val text = (spark.readStream\n",
    "    .option(\"maxFilesPerTrigger\", 1)  // fake streaming read one file per trigger\n",
    "    .text(\"data/othello/part*\")\n",
    "    .as[String])\n",
    "    \n",
    "// Count words\n",
    "val counts = (text.flatMap(line => line.split(\"\\\\s+\"))\n",
    "    .groupByKey(_.toLowerCase)\n",
    "    .count)\n",
    "    \n",
    "// Print counts\n",
    "val query = (counts\n",
    "    .orderBy($\"count(1)\" desc)\n",
    "    .writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .format(\"console\")\n",
    "    .start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/logo-text.jpg\" width=\"20%\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
