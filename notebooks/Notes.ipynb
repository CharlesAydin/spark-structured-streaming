{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sqlC = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlC.implicits._\n",
    "val spark2 = spark\n",
    "import spark2.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val spark2 = spark\n",
    "import spark2.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:34: error: constructor SparkSession in class SparkSession cannot be accessed in class $iw\n",
       "       val spark2 = new org.apache.spark.sql.SparkSession(sc)\n",
       "                    ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark2 = new org.apache.spark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[2] at map at <console>:29"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// RDD\n",
    "\n",
    "val rdd = (sc.textFile(\"notebooks/data/people.csv\")\n",
    "    .map(_.split(\",\")))\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "|  26|\n",
      "|  15|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Dataset\n",
    "\n",
    "val df = spark.read.json(\"notebooks/data/people.json\")\n",
    "df.select($\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys.process._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Michael\"}\n",
      "{\"name\":\"Andy\", \"age\":30}\n",
      "{\"name\":\"Justin\", \"age\":19}\n",
      "{\"name\":\"Justin\", \"age\":26}\n",
      "{\"name\":\"Justin\", \"age\":15}\n"
     ]
    }
   ],
   "source": [
    "\"more notebooks/data/people.json\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "|  26|\n",
      "|  15|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case class Person(name: String, age: Option[BigInt])\n",
    "val ds = spark2.read.json(\"notebooks/data/people.json\").as[Person]\n",
    "ds.select($\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:49: error: Unable to find encoder for type stored in a Dataset.  Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.\n",
       "       ds.map(_.age.getOrElse(0)).sum()\n",
       "             ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.map(_.age.getOrElse(0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataset = (0 to 9).toDS\n",
    "dataset.filter(_ % 2 == 0).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('id = 0)\n",
      "+- Range (0, 10, splits=2)\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint\n",
      "Filter (id#71L = cast(0 as bigint))\n",
      "+- Range (0, 10, splits=2)\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (id#71L = 0)\n",
      "+- Range (0, 10, splits=2)\n",
      "\n",
      "== Physical Plan ==\n",
      "*Filter (id#71L = 0)\n",
      "+- *Range (0, 10, splits=2)\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).filter('id === 0).explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val million = 0 until math.pow(10, 6).toInt\n",
    "sc.parallelize(million).cache.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "million.toDF.cache.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "case class Foo(i: Int, s: String, d: Double)\n",
    "val data = (0 to 10).map(x => Foo(x, x.toString, x * 3.14)).toDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"i\":0,\"s\":\"0\",\"d...|\n",
      "|{\"i\":1,\"s\":\"1\",\"d...|\n",
      "|{\"i\":2,\"s\":\"2\",\"d...|\n",
      "|{\"i\":3,\"s\":\"3\",\"d...|\n",
      "|{\"i\":4,\"s\":\"4\",\"d...|\n",
      "|{\"i\":5,\"s\":\"5\",\"d...|\n",
      "|{\"i\":6,\"s\":\"6\",\"d...|\n",
      "|{\"i\":7,\"s\":\"7\",\"d...|\n",
      "|{\"i\":8,\"s\":\"8\",\"d...|\n",
      "|{\"i\":9,\"s\":\"9\",\"d...|\n",
      "|{\"i\":10,\"s\":\"10\",...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.toJSON.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sqlC = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlC.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[2] at map at <console>:22"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// RDD\n",
    "\n",
    "val rdd = (sc.textFile(\"data/people.csv\")\n",
    "    .map(_.split(\",\")))\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "| Justin|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Dataset\n",
    "\n",
    "val df = spark.read.json(\"data/people.json\")\n",
    "df.select($\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataset = (0 to 9).toDS\n",
    "dataset.filter(_ % 2 == 0).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.range(10).filter('id === 0).explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Performance\n",
    "\n",
    "Run these two jobs and goto localhost:4040\n",
    "\n",
    "https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-tungsten.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val million = 0 until math.pow(10, 6).toInt\n",
    "sc.parallelize(million).cache.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "million.toDF.cache.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[age: bigint, count: bigint]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = (spark.read\n",
    "  .option(\"header\", true)\n",
    "  .option(\"inferSchema\", true)\n",
    "  .json(\"data/people.json\"))\n",
    "  \n",
    "df.groupBy(\"age\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data from Implied Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val explicitSchema = StructType(\n",
    "  StructField(\"name\", StringType, false) ::\n",
    "  StructField(\"city\", StringType, true) ::\n",
    "  StructField(\"country\", StringType, true) ::\n",
    "  StructField(\"age\", IntegerType, true) :: Nil\n",
    ")\n",
    "\n",
    "val inExplicit = (spark.readStream\n",
    "  .schema(explicitSchema)\n",
    "  .format(\"csv\")\n",
    "  .option(\"header\", true)\n",
    "  .option(\"maxFilesPerTrigger\", 1)\n",
    "  .load(\"data/people.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown\n",
       "Message: Unable to retrieve error!\n",
       "StackTrace: "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inImplied = (spark.read\n",
    "  .format(\"csv\")\n",
    "  .option(\"header\", true)\n",
    "  .option(\"inferSchema\", true)\n",
    "  .load(\"data/people.csv\"))\n",
    "  \n",
    "val impliedSchema = inImplied.schema\n",
    "\n",
    "println(impliedSchema)\n",
    "println(explicitSchema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
