{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sqlC = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlC.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[41] at map at <console>:32"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// RDD\n",
    "\n",
    "val rdd = (sc.textFile(\"data/people.csv\")\n",
    "    .map(_.split(\",\")))\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "| Justin|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Dataset\n",
    "\n",
    "val df = spark.read.json(\"data/people.json\")\n",
    "df.select($\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataset = (0 to 9).toDS\n",
    "dataset.filter(_ % 2 == 0).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('id = 0)\n",
      "+- Range (0, 10, splits=2)\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint\n",
      "Filter (id#71L = cast(0 as bigint))\n",
      "+- Range (0, 10, splits=2)\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (id#71L = 0)\n",
      "+- Range (0, 10, splits=2)\n",
      "\n",
      "== Physical Plan ==\n",
      "*Filter (id#71L = 0)\n",
      "+- *Range (0, 10, splits=2)\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).filter('id === 0).explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val million = 0 until math.pow(10, 6).toInt\n",
    "sc.parallelize(million).cache.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "million.toDF.cache.count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
