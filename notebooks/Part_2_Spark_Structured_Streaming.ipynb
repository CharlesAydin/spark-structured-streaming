{
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "# Spark Structured Streaming\n",
      "\n",
      "Recall that we can think of Spark Structured Streaming as generating an infinite dataset against which we write queries.  We'll explore the API fully\n",
      "\n",
      "- We'll demonstrate how to pull data from different streaming sources like websockets or files.\n",
      "- We'll delve into parsing and structuring data.\n",
      "- We'll demonstrate how to run queries against this data using the structured streaming API."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "execution_count": null,
     "input": [
      "val sparkDummy = spark\n",
      "import sparkDummy.implicits._"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Socket Structured Streaming Example\n",
      "\n",
      "We're going to stream one of Shakespeare's most famous poems on a fixed port and listen for it using Spark.  We setup a streaming server to broadcast the file one line at a time using [Broadcast.scala](/edit/Broadcast.scala)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "import sys.process._\n",
      "\n",
      "\"more data/summer.txt\" ! // run bash command using bang after a string"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "// create a stream and listen on a port\n",
      "\n",
      "def createStream(port: Int, duration: Int) {\n",
      "    val lines = (spark.readStream\n",
      "        .format(\"socket\")\n",
      "        .option(\"host\", \"localhost\")\n",
      "        .option(\"port\", port)\n",
      "        .load())\n",
      "\n",
      "    val words = (lines\n",
      "        .as[String]\n",
      "        .flatMap(_.split(\"\\\\s+\")))\n",
      "\n",
      "    val wordCounts = (words\n",
      "        .groupByKey(_.toLowerCase)\n",
      "        .count()\n",
      "        .orderBy($\"count(1)\" desc))\n",
      "\n",
      "    val query = (wordCounts.writeStream\n",
      "        .outputMode(\"complete\")\n",
      "        .format(\"console\")\n",
      "        .start\n",
      "        .awaitTermination(duration))\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "val port = 9001\n",
      "\n",
      "// Broadcast file on port one line at time\n",
      "(new Thread {\n",
      "    override def run {\n",
      "        s\"scala Broadcast.scala ${port} data/summer.txt\" !\n",
      "    }\n",
      "}).start"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "createStream(port, 12000)"
     ],
     "language": "python",
     "metadata": {
      "scrolled": false
     },
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Netcast Socket Structured Streaming Example\n",
      "\n",
      "We can also broadcast on Unix's `netcast` to broadcast a stream on a fixed port."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "// run `nc -lk 9002` in bash and start typing!\n",
      "\n",
      "createStream(9002, 10000)"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Spark Structured Streaming Parsing Data\n",
      "\n",
      "Much as with datasets, we can use a `case class` to represent rows of data.  The case classe's attributes correspond to the json field names or (as in this case) the CSV column names.\n",
      "\n",
      "However, unlike with datasets, we cannot ask the reader to infer the schema.  Instead, we will use `ScalaReflection` to generate a schema for our case class."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "import sys.process._\n",
      "\n",
      "\"cat data/people/1.csv\" ! // run bash command using bang after a string"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.catalyst.ScalaReflection\n",
      "\n",
      "case class Person(\n",
      "    name: String,\n",
      "    city: String,\n",
      "    country: String,\n",
      "    age: Option[Int]\n",
      ")\n",
      "\n",
      "// create schema for parsing data\n",
      "val caseSchema = (ScalaReflection\n",
      "    .schemaFor[Person]\n",
      "    .dataType\n",
      "    .asInstanceOf[StructType])\n",
      "\n",
      "val peopleStream = (spark.readStream\n",
      "    .schema(caseSchema)\n",
      "    .option(\"header\", true)  // Headers are matched to Person properties\n",
      "    .option(\"maxFilesPerTrigger\", 1)  // each file is read in a separate batch\n",
      "    .csv(\"data/people/\")\n",
      "    .as[Person])\n",
      "  \n",
      "(peopleStream.writeStream\n",
      "    .outputMode(\"append\")  // write results to screen\n",
      "    .format(\"console\")\n",
      "    .start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise:** What would happen if age were not optional?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Constructing Columns in Structured Streaming\n",
      "\n",
      "Datasets use a dataframe syntax to refer to columns (which are themselves `Column` objects).  There are a number of ways to do this:\n",
      "- `peopleStream(\"country\")`\n",
      "- `peopleStream.col(\"country\")`\n",
      "- `$\"country\"`\n",
      "- `'country`\n",
      "\n",
      "The first two are more explicit as they tell Spark which data stream to use.  This is useful in joins when we want to specify the table more explicitly.  The second two are more implicit as they do not specify the data stream.  These are more useful for single datastream operations.  The symbols need to be imported from `spark.implicits`.\n",
      "\n",
      "There are actually multiple ways to construct columns:\n",
      "- The above allows us to refernce `Column`s already in a dataframe.\n",
      "- We can also construct a `Column` from other `Column`s using binary operators like `===` (equality), `>`, `<=`, `.plus`, `-`, `.startsWith`, or `&&`, depending on the underlying value of the column.\n",
      "- Finally, we can rename the columns (keeping the values) with the operator `as`."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Selecting Columns Using Structured Streaming\n",
      "\n",
      "We'll demonstrate these using the `select` method, which takes any non-zero number of `Column` arguements and returns a dataframe with those arguements."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "(peopleStream.select(\n",
      "    $\"country\" === \"UK\" as \"in_UK\",\n",
      "    $\"age\" <= 30 as \"under_30\",\n",
      "    $\"age\" + 1,\n",
      "    ($\"city\" === \"London\") && ($\"age\" <= 40) as \"young_Londoner\",\n",
      "    'country startsWith \"U\" as \"U_Country\")\n",
      "        .writeStream\n",
      "        .outputMode(\"append\")  // write results to screen\n",
      "        .format(\"console\")\n",
      "        .start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Filtering Rows in Structured Streaming\n",
      "\n",
      "Filter takes a boolean-valued `Column` and returns a dataframe whose rows are the values where only the rows for which the column is true are kept."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "(peopleStream.filter($\"age\" === 22)\n",
      "    .writeStream\n",
      "    .outputMode(\"append\")  // write results to screen\n",
      "    .format(\"console\")\n",
      "    .start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "**Exercises:**\n",
      "\n",
      "1. Filter for when the age is no less than 22.\n",
      "1. Filter for the city being \"London\"\n",
      "1. Filter for Americans under the age of 30"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Groupby in Structured Streaming\n",
      "\n",
      "We can use groupby and aggregation as we would in SQL.\n",
      "\n",
      "- `groupBy` takes one or more `Column`s along which to groupBy.\n",
      "- The resulting object supports various builtin aggregation funcitons (`avg`, `mean`, `min`, `max`, `sum`) which take one or more string column names along which to aggregate."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "(peopleStream.groupBy('country)\n",
      "    .mean(\"age\")\n",
      "    .writeStream\n",
      "    .outputMode(\"complete\")\n",
      "    .format(\"console\")\n",
      "    .start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Groupby Aggregations in Structured Streaming\n",
      "\n",
      "For more complex aggregations, we can use `.agg`, which takes columns with aggregations.  Notice that we can reuse the keyword `as`, as well as other binary column operators from before."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "(peopleStream.groupBy('city)\n",
      "    .agg(first(\"country\") as \"country\", count(\"age\"))\n",
      "    .writeStream\n",
      "    .outputMode(\"complete\")\n",
      "    .format(\"console\")\n",
      "    .start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise:** Add the average age of each city to the above query"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Joining Structured Stream with Datasets\n",
      "\n",
      "We can join datastreams with datasets.  Remember: both of these are distributed datasets and one is being streamed -- that's a lot of semantics for a simple `.join` operator!\n",
      "\n",
      "Below, we take a fixed user table and join it in with a stream of transactions in a fictitious poultry ecommerce website."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "case class User(id: Int, name: String, email: String, country: String)\n",
      "case class Transaction(userid: Int, product: String, cost: Double)\n",
      "\n",
      "// A user dataset\n",
      "// Notice that we do not have to provide a schema\n",
      "// We can simply infer it\n",
      "val users = (spark.read\n",
      "    .option(\"inferSchema\", \"true\")\n",
      "    .option(\"header\", true)\n",
      "    .csv(\"data/users.csv\")\n",
      "    .as[User]\n",
      ")\n",
      "\n",
      "val transactionSchema = (ScalaReflection\n",
      "    .schemaFor[Transaction]\n",
      "    .dataType\n",
      "    .asInstanceOf[StructType]\n",
      ")\n",
      "  \n",
      "// A stream of transactions\n",
      "val transactionStream = (spark.readStream\n",
      "    .schema(transactionSchema)\n",
      "    .option(\"header\", true)\n",
      "    .option(\"maxFilesPerTrigger\", 1)\n",
      "    .csv(\"data/transactions/*.csv\")\n",
      "    .as[Transaction]\n",
      ")\n",
      "\n",
      "// Join transaction stream with user dataset\n",
      "val spendingByCountry = (transactionStream\n",
      "    .join(users, users(\"id\") === transactionStream(\"userid\"))\n",
      "    .groupBy($\"country\")\n",
      "    .agg(sum($\"cost\")) as \"spending\")\n",
      "    \n",
      "// Print result\n",
      "(spendingByCountry.writeStream\n",
      "    .outputMode(\"complete\")\n",
      "    .format(\"console\")\n",
      "    .start)"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercises:**\n",
      "- Show sales by product rather than country.\n",
      "- Show sales by both product and country."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# SQL Queries in Spark Structured Streaming\n",
      "\n",
      "Finally we can use the method `createOrReplaceTempView` to publish streams (and static datasets) as SQL tables.  We can then query the resulting table using SQL and stream the output as we would with any other datastream."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "// Publish SQL table\n",
      "peopleStream.createOrReplaceTempView(\"peopleTable\")\n",
      "\n",
      "// SQL query\n",
      "val query = spark.sql(\"SELECT country, avg(age) FROM peopleTable GROUP BY country\")\n",
      "\n",
      "// Output\n",
      "(query.writeStream\n",
      "    .outputMode(\"complete\")\n",
      "    .format(\"console\")\n",
      "    .start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 88
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise**:\n",
      "- Use the SQL syntax to filter for londoners under 40 years.\n",
      "- Use the SQL syntax to join the user table and transaction stream to get transactions by country and product."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"images/logo-text.jpg\" width=\"20%\"/>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}