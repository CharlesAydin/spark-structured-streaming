{
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": true,
     "execution_count": null,
     "input": [
      "val sparkDummy = spark\n",
      "import sparkDummy.implicits._"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Comparing Structured Streaming with DStream\n",
      "\n",
      "The old spark streaming API used discretized streams or DStreams.  The model was that rather than a single, ever-growing dataset, a discretized stream provided an RDD at each trigger.  In a sense, it was a series of discretized batch operations.\n",
      "\n",
      "![](images/streaming-flow.png)\n",
      "\n",
      "<!--- Images from http://spark.apache.org/docs/latest/streaming-programming-guide.html --->\n",
      "\n",
      "The model had several drawbacks:\n",
      "1. DStream made it hard to deal with late or out of order data because the DStream was just discretized batches so it was difficult to update an old batch.\n",
      "1. The API for DStream was very different from the API for RDDs because their underlying data model was different.\n",
      "1. The streaming followed-by batch semantics made reliability hard.  If one step failed, the semantics of rerunning were unclear.\n",
      "\n",
      "Nonetheless, it is still a more built out than Structured Streaming and the API has feature support.  DStream will play a role in your streaming Spark applications for the foreseable future."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Custom Receivers in Spark DStream\n",
      "\n",
      "DStreams have support for custom receivers (i.e. sources).  For example, we are able to create a timed file source, which uses a separate thread to read data from a file and transmit the contents line by line, one each second."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "import scala.io.Source\n",
      "import org.apache.spark.streaming.receiver.Receiver\n",
      "import org.apache.spark.storage.StorageLevel\n",
      "\n",
      "// Should be a class ...\n",
      "def timedFileSource(fileName: String) = {\n",
      "    new Receiver[String](StorageLevel.MEMORY_AND_DISK_2) {\n",
      "        def onStart() {\n",
      "            new Thread(\"Timed File Source\") {\n",
      "                override def run() { receive() }\n",
      "            }.start()\n",
      "        }\n",
      "\n",
      "        def onStop() { }\n",
      "\n",
      "        private def receive() {\n",
      "            for (line <- Source.fromFile(fileName).getLines) {\n",
      "                println(line)  // print for debugging\n",
      "                store(line)  // send the line as a source\n",
      "                Thread.sleep(1000L)  // Wait one second\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Iterative Wordcount using Spark DStream\n",
      "\n",
      "To illustrate the difference in APIs, we will write wordcount using DStream.  As we'll see, the syntax appears similar but the semantics are vastly different.\n",
      "![dstream-ops](images/streaming-dstream.png)\n",
      "\n",
      "Because each discretized RDD is its own entity, we are only see words in each time period and the wordcount will be per period, not the cumulative wordcount.\n",
      "![dstream-ops](images/streaming-dstream-ops.png)\n",
      "\n",
      "<!--- Images from http://spark.apache.org/docs/latest/streaming-programming-guide.html --->"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "execution_count": null,
     "input": [
      "import org.apache.spark._\n",
      "import org.apache.spark.streaming._\n",
      "\n",
      "val ssc = new StreamingContext(sc, Seconds(1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "(ssc.receiverStream(timedFileSource(\"data/summer.txt\"))\n",
      "    .flatMap(_.split(\" \"))\n",
      "    .map(word => (word, 1))\n",
      "    .reduceByKey(_ + _)\n",
      "    .print())\n",
      "\n",
      "ssc.start()             // Start the computation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Cumulative Wordcount using Spark DStream\n",
      "\n",
      "The above only gives the iterative wordcount so how do we get the cumulative wordcount?  It turns out we need to keep track of state using separate `State` variables and manually update those states in calls to `mapWithState`.  The API is definitely more involved and more cumbersome."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "// You cannot create jobs in a started streaming context\n",
      "// Consider restarting the kernel\n",
      "\n",
      "def updateFn(key: String, value: Option[Int], state: State[Int]) = {\n",
      "    // update state (stateful!)\n",
      "    state.update(value.getOrElse(0) + state.getOption.getOrElse(0))\n",
      "\n",
      "    // result to return\n",
      "    (key, state.get)\n",
      "}\n",
      "\n",
      "val spec = StateSpec.function(updateFn _)\n",
      "\n",
      "// checkpointing is mandatory\n",
      "ssc.checkpoint(\"_checkpoints\")\n",
      "\n",
      "(ssc.receiverStream(timedFileSource(\"data/summer.txt\"))\n",
      "    .flatMap(_.split(\" \"))\n",
      "    .map(word => (word, 1))\n",
      "    .mapWithState(spec)\n",
      "    .print())\n",
      "\n",
      "ssc.start()             // Start the computation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Benefits of Spark Tungsten\n",
      "\n",
      "We've seen the surface-level API differences between DStream and Structured Streaming, there are also many differences under the hood.  These have been added as newer versions of Spark for Datasets and Structured Streaming and are sometimes retrofitted to RDDs.\n",
      "\n",
      "While Spark has traditionally been focused on optimizing for inter-computer network IO Efficiency.  This was the major bottle neck in mapreduce and other traditional distributed computing systems.  At this point, Spark is now intra-computer memory and CPU bound.\n",
      "\n",
      "1. **Customized Memory Management:** Spark understands its own memory allocation needs better than the generic JVM garbage collector.  Tungsten takes advantage of this by using `sun.misc.Unsafe`, which exposes C-Style off-heap memory access.  The result is fewer unecessary garbage colleciton events and improved performance.\n",
      "1. **Binary Encoding:** Spark traditionally needed to serialized data to JVM objects, it now uses the Tungsten binary encoding.  This has two major advantages.  First, it decreases the memory footprint.  While \"ABCD\" would take 4 bytes of UTF-8 to encode, it would be stored using 48 bytes as a JVM object.  Second, instead of serializing into objects, it's able to perform many actions directly on the the raw binary encoding, reducing the computational overhead for serialization / deserialization.\n",
      "1. **Code Generation:** Spark historically used generic JVM function evaluation.  Given the virtual function lookups, automated boxing of primitive types, and other JVM overhead, this dramatically slows down the computation.  By using type information, Tungsten is able to generate byte code and speed up performance.\n",
      "1. **Cache-aware Computation:** Tungsten lays out its memory in a way that takes advantage of CPU cache locality to reduce cache spilling and speed up computations.\n",
      "\n",
      "For more information, check out [this blog post](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html) or [this presentation](http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Tungsten Performance Benefit Demonstration\n",
      "\n",
      "The following two operations count a million integers in memory.\n",
      "- The first uses RDDs and must serialize all the objects into Java Objects.\n",
      "- The second use sTungsten and uses a more compact binary encoding.\n",
      "\n",
      "Tungsten saves a factor of nearly 4 on memory:\n",
      "\n",
      "![Tungsten Memory](images/Tungsten_Memory.png)\n",
      "\n",
      "To reproduce the example, run the following code and goto your Spark UI Viewer.  You can launch a new shell using `make spark-shell` (recommended), this will be at [http://localhost:5050/storage/](http://localhost:5050/storage/).  The default Spark UI port is 4040."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "val million = sc.parallelize(0 until math.pow(10, 6).toInt)\n",
      "\n",
      "// Using RDDs\n",
      "million.cache.count\n",
      "\n",
      "// Using Tungsten\n",
      "million.toDS.cache.count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Benefits of Spark Catalyst\n",
      "\n",
      "Optimizations include:\n",
      "1. **Constant Folding:** Evaluating constant expressions at compile time rather than at run time.\n",
      "1. **Predicate Pushdown:** Running operations that reduce the dataload (e.g. selecting columns or filtering rows) earlier in the query.  This reduces the amount of data that needs to be processed.\n",
      "1. **Projection Pruning:** Only reading the columns (fields) used in the query from the database.\n",
      "1. **Pipelining Operations:** Combines multiple projection and filter operations into a single map operation.\n",
      "1. **Cost-based Optimization:** Spark actually builds multiple plans to compute a query, computes their cost, and chooses the cheapest one.\n",
      "1. **Code Generation:** The final operational plan is transformed into optimized Java bytecode."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Viewing Query Plans in Spark Shell\n",
      "\n",
      "For an RDD, we can view the query plan calling the `.toDebugString` method.  Notice that multiple (even consecutive!) maps and filters are run as separate map and filter steps."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "(million.map(_ + 1)\n",
      "    .map(_ + 1)\n",
      "    .filter(_ > 1)\n",
      "    .filter(_ > 2 * 4)\n",
      "    .toDebugString)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Visualizing Query Stages in Spark UI Viewer\n",
      "\n",
      "You can also checkout the \"Stages\" tab in the Spark UI viewer to checkout the individual stages\n",
      "\n",
      "<img src=\"images/spark-stages.png\" width=\"40%\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Viewing Spark Catalyst-Optimzied Physical Plans\n",
      "\n",
      "For a dataset, we can view the query plan with the `.explain` method.  You can see that it's performed several optimizations:\n",
      "1. It's grouped multiple (even non-consecutive!) `select` and `filter` operations together\n",
      "1. It's pushed the `.filter` earlier in the query\n",
      "1. It performs constant folding by multiplying out `2 * 4`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "execution_count": null,
     "input": [
      "(million.toDS\n",
      "    .select('value, 'value + 1 as 'value2)\n",
      "    .select('value2 + 1 as 'value2)\n",
      "    .filter('value >= 1)\n",
      "    .select('value2 + 1 as 'value2)\n",
      "    .filter('value >= 2 * 4)\n",
      "    .explain)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"images/logo-text.jpg\" width=\"20%\"/>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}